# -*- coding: utf-8 -*-
"""Copy of wordvec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w5qkXOKnAlWQtBCoRTjXDD58cShQ_M-3
"""

#importing modules


import nltk
nltk.download('all',halt_on_error = False)
!pip install gensim

import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns

import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize,sent_tokenize
from nltk.stem import PorterStemmer,LancasterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer

from nltk import pos_tag
from nltk import ne_chunk

from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer

from bs4 import BeautifulSoup
import re
from sklearn.model_selection import train_test_split,cross_validate
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import accuracy_score,roc_auc_score
from sklearn.metrics import classification_report

from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC,SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB,MultinomialNB

stop_words = set(nltk.corpus.stopwords.words('english'))

import keras
from keras.preprocessing.text import one_hot,Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense , Flatten ,Embedding,Input
from keras.models import Model
from keras.preprocessing.text import text_to_word_sequence

from gensim.models import Word2Vec

#loading the dataset

train = pd.read_csv('/content/labeledTrainData.tsv',header=0,\
                   delimiter="\t",quoting=3)

test = pd.read_csv('/content/testData.tsv',header=0,\
                  delimiter="\t",quoting=3)

train.shape

test.shape

df = train.copy()

df.head()

df.drop('id',axis=1,inplace=True)

df.head()

df['sentiment'].value_counts()

df['review'][3]

#data cleaning and preprocessing

def clean_reviews(review):
  review_text = BeautifulSoup(review).get_text()            #remove html tags
  review_text = re.sub("[^a-zA-Z]"," ",review_text)         #remove all characters except alphabets
  word_tokens = review_text.lower().split()                 #convert it into lower case and split each word
  stop_words = set(stopwords.words('english'))              #remove stop words
  word_tokens = [w for w in word_tokens if not w in stop_words]

  cleaned_review = " ".join(word_tokens)
  return cleaned_review



all_reviews=df['review']+test['review']
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
sentences=[]
sum=0
for review in all_reviews:
  sents=tokenizer.tokenize(review.strip())
  sum+=len(sents)
  for sent in sents:
    cleaned_sent=clean_reviews(sent)
    sentences.append(cleaned_sent.split())
print(sum)
print(len(sentences))

for i in range(10): # just printing few sentences.
  print(sentences[i])

w2v_model = Word2Vec(sentences,size =300 ,window =3, min_count =1)
w2v_model.train(sentences,total_examples = len(sentences),epochs = 40)

w2v_model.wv.get_vector('sad')

w2v_model.wv.most_similar('sad')

w2v_model.wv.similarity('happy','sad')

df['review'] = df['review'].apply(clean_reviews)
test['review'] = test['review'].apply(clean_reviews)

def make_doc_vectors(review):
  doc_vec=np.zeros(300,dtype='float64') 
  words=review.split()
  for word in words: 
    if word in w2v_model.wv.vocab:
      word_vec=w2v_model.wv.get_vector(word) 
      doc_vec=doc_vec+word_vec
  return doc_vec/len(words)

doc_vectors_train=[]
for review in df['review']:
  doc_vectors_train.append(make_doc_vectors(review))
doc_vectors_train=np.array(doc_vectors_train)

doc_vectors_train

doc_vectors_train.shape

Y=np.array(list(df['sentiment']))

Y.shape

x_train,x_test,y_train,y_test = train_test_split(doc_vectors_train,Y,test_size= 0.2,random_state = 42)

y_test.shape

clf_lr = LogisticRegression(C=1)
clf_lr.fit(x_train,y_train)
pred=clf_lr.predict(x_test)

pred.mean()

y_train.mean()

print(accuracy_score(pred,y_test))

